{
  simple_game: { # the simple game settings
    size: 5, # the size of the grid used
    rocks: [ # the rocks locations
      [1, 2],
      [3, 3],
      [2, 4]
    ],
    transmiter_stations: [ # the transmission station locations
      [4, 4]
    ],
    cliffs: [ # the cliff locations
      [2, 3],
      [1, 1]
    ],
    uphills: [ # the uphill locations
      [0, 4],
      [2, 0]
    ],
    downhills: [ # the downhill locations
      [3, 0],
      [0, 2]
    ],
    battery_stations: [ # the battery station location
      [4, 2]  
    ]
  },
  deep_q_game: { # the deep q game settings
    params: { # the list of parameters to use
        "num_episodes":    2000, # the length of the epsiode
        "block_length":    100, # the length of each block       
        "max_steps":       250, # the max steps the robot can take
        "target_update":   100, # how often the target network is updated
        "memory_capacity": 20000, # the capacity of the replay buffer
        "batch_size":      256, # the batch size for the neural network
        "hidden_size":     512, # the size of the hidden layer in the neural network
        "learning_rate":   5e-4, # the learning rate used for the optimiser
        "gamma":           0.99, # the gamma discount factor for future rewards
        "epsilon_start":   1.0, # the starting epsilon value, if using epsilon greedy policy
        "epsilon_decay":   0.995, # the epsilon decay rate, if using epsilon greedy policy
        "epsilon_min":     0.05, # the final epsilon valeue, if using epsilon greedy policy
        "tau":             0.005, # tau is used to make target updates less aggressive
        "prioritised":     False, # if prioritised experience replay, PER, is needed
        "per_alpha":       0.6, # for prioritised exponent for PER
        "per_beta":        0.4, # for importance sampling for PER
        "double_dqn":      False, # if double deep-q network is needed
        "save_checkpoint": True, # if the weights need to be saved after training is complete
        "policy_type":     "softmax" # the policy: either epsilon-greedy or softmax
    },
    size: 5, # the size of the grid
    rocks: [ # the rock locations
      [1, 2],
      [3, 3],
      [2, 4]
    ],
    transmitter_stations: [ # the transmitter station locations
      [4, 4]
    ],
    cliffs: [ # the cliffs locations
      [2, 3],
      [1, 1]
    ],
    uphills: [ # the uphill locations
      [0, 4],
      [2, 0]
    ],
    downhills: [ # the downhill locations
      [3, 0],
      [0, 2]
    ],
    battery_stations: [ # the battery station locations
      [4, 2]  
    ]
  },
}
